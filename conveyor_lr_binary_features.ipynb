{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49ed370f",
   "metadata": {},
   "source": [
    "\n",
    "# ConveyorAI — Confidence Score via Logistic Regression\n",
    "\n",
    "This simplified notebook removes asyncio. It:\n",
    "1) Loads the Digisign dataset\n",
    "2) Uses **one synchronous LLM call per row** to extract **binary failure‑mode features**\n",
    "3) Labels: **1 only** if `conveyor_ai_grade == \"perfect\"`, else 0\n",
    "4) Trains **Logistic Regression** on the full dataset\n",
    "5) Shows a **confusion matrix**\n",
    "6) Plots **predicted p(accurate)** vs the original categorical grade\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c4fbc0",
   "metadata": {},
   "source": [
    "## 0) Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "04440cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM client initialized: True | Model: gpt-5-nano\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os, json, time, difflib\n",
    "from typing import Any, Dict, List\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "try:\n",
    "    from openai import OpenAI\n",
    "except Exception:\n",
    "    OpenAI = None\n",
    "\n",
    "# --- Paths & Columns ---\n",
    "DATA_PATH = \"human_eval_data.csv\"\n",
    "ID_COL = \"id\"\n",
    "QUESTION_COL = \"question\"\n",
    "AI_ANSWER_COL = \"conveyor_ai_answer\"\n",
    "GRADE_COL = \"conveyor_ai_grade\"\n",
    "SOURCE_PREFIX = \"source_\"\n",
    "\n",
    "# --- LLM Config (sync) ---\n",
    "MODEL_NAME = os.getenv(\"LLM_MODEL_NAME\", \"gpt-5-nano\")\n",
    "TEMPERATURE = float(os.getenv(\"LLM_TEMPERATURE\", \"0.0\"))\n",
    "SLEEP_BETWEEN_SECONDS = float(os.getenv(\"SLEEP_BETWEEN_SECONDS\", \"0.0\"))  # add delay between requests if needed\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "client = OpenAI(api_key=OPENAI_API_KEY) if (OPENAI_API_KEY and OpenAI) else None\n",
    "print(\"LLM client initialized:\", bool(client), \"| Model:\", MODEL_NAME)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d738ac63",
   "metadata": {},
   "source": [
    "## 1) Load dataset & gather sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3b802f0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 207 Cols: ['id', 'questionnaire', 'question', 'conveyor_ai_answer', 'user_updated_answer', 'conveyor_ai_grade', 'num_sources', 'source_1', 'source_2', 'source_3', 'source_4', 'source_5', 'source_6', 'source_7', 'source_8', 'source_9', 'source_10', 'source_11']\n",
      "Detected source columns: ['source_1', 'source_2', 'source_3', 'source_4', 'source_5', 'source_6', 'source_7', 'source_8', 'source_9', 'source_10', 'source_11']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>questionnaire</th>\n",
       "      <th>question</th>\n",
       "      <th>conveyor_ai_answer</th>\n",
       "      <th>user_updated_answer</th>\n",
       "      <th>conveyor_ai_grade</th>\n",
       "      <th>num_sources</th>\n",
       "      <th>source_1</th>\n",
       "      <th>source_2</th>\n",
       "      <th>source_3</th>\n",
       "      <th>source_4</th>\n",
       "      <th>source_5</th>\n",
       "      <th>source_6</th>\n",
       "      <th>source_7</th>\n",
       "      <th>source_8</th>\n",
       "      <th>source_9</th>\n",
       "      <th>source_10</th>\n",
       "      <th>source_11</th>\n",
       "      <th>sources_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NJMEL_1</td>\n",
       "      <td>NJMEL</td>\n",
       "      <td>Q1-  Does the organization have a dedicated in...</td>\n",
       "      <td>Yes. The organization has a dedicated Security...</td>\n",
       "      <td>Yes. We have a dedicated Security team respons...</td>\n",
       "      <td>accurate_imperfect</td>\n",
       "      <td>3</td>\n",
       "      <td>Do you have a dedicated Information Security s...</td>\n",
       "      <td>Who is responsible for cybersecurity within th...</td>\n",
       "      <td>Digisign Security Incident Response Plan\\n----...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Do you have a dedicated Information Security ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NJMEL_2</td>\n",
       "      <td>NJMEL</td>\n",
       "      <td>Q2-  Does your organization conduct risk asses...</td>\n",
       "      <td>Yes. Risk assessments are conducted at least a...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>perfect</td>\n",
       "      <td>3</td>\n",
       "      <td>Are cyber security risk assessments performed ...</td>\n",
       "      <td>Digisign SOC 2 Type II report\\n-----\\nRisk Ass...</td>\n",
       "      <td>Digisign Enterprise Information Security Polic...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Are cyber security risk assessments performed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NJMEL_3</td>\n",
       "      <td>NJMEL</td>\n",
       "      <td>Q3-  Are background and credit checks conducte...</td>\n",
       "      <td>Yes. Background checks, including federal and ...</td>\n",
       "      <td>Yes. Background checks, including federal and ...</td>\n",
       "      <td>inaccurate</td>\n",
       "      <td>2</td>\n",
       "      <td>Digisign SOC 2 Type II report\\n-----\\nPrior to...</td>\n",
       "      <td>Does your company have an employee pre-screeni...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Digisign SOC 2 Type II report\\n-----\\nPrior t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id questionnaire                                           question  \\\n",
       "0  NJMEL_1         NJMEL  Q1-  Does the organization have a dedicated in...   \n",
       "1  NJMEL_2         NJMEL  Q2-  Does your organization conduct risk asses...   \n",
       "2  NJMEL_3         NJMEL  Q3-  Are background and credit checks conducte...   \n",
       "\n",
       "                                  conveyor_ai_answer  \\\n",
       "0  Yes. The organization has a dedicated Security...   \n",
       "1  Yes. Risk assessments are conducted at least a...   \n",
       "2  Yes. Background checks, including federal and ...   \n",
       "\n",
       "                                 user_updated_answer   conveyor_ai_grade  \\\n",
       "0  Yes. We have a dedicated Security team respons...  accurate_imperfect   \n",
       "1                                                NaN             perfect   \n",
       "2  Yes. Background checks, including federal and ...          inaccurate   \n",
       "\n",
       "   num_sources                                           source_1  \\\n",
       "0            3  Do you have a dedicated Information Security s...   \n",
       "1            3  Are cyber security risk assessments performed ...   \n",
       "2            2  Digisign SOC 2 Type II report\\n-----\\nPrior to...   \n",
       "\n",
       "                                            source_2  \\\n",
       "0  Who is responsible for cybersecurity within th...   \n",
       "1  Digisign SOC 2 Type II report\\n-----\\nRisk Ass...   \n",
       "2  Does your company have an employee pre-screeni...   \n",
       "\n",
       "                                            source_3 source_4 source_5  \\\n",
       "0  Digisign Security Incident Response Plan\\n----...      NaN      NaN   \n",
       "1  Digisign Enterprise Information Security Polic...      NaN      NaN   \n",
       "2                                                NaN      NaN      NaN   \n",
       "\n",
       "  source_6 source_7 source_8 source_9 source_10 source_11  \\\n",
       "0      NaN      NaN      NaN      NaN       NaN       NaN   \n",
       "1      NaN      NaN      NaN      NaN       NaN       NaN   \n",
       "2      NaN      NaN      NaN      NaN       NaN       NaN   \n",
       "\n",
       "                                        sources_list  \n",
       "0  [Do you have a dedicated Information Security ...  \n",
       "1  [Are cyber security risk assessments performed...  \n",
       "2  [Digisign SOC 2 Type II report\\n-----\\nPrior t...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "print(\"Rows:\", len(df), \"Cols:\", list(df.columns))\n",
    "\n",
    "source_cols = [c for c in df.columns if c.startswith(SOURCE_PREFIX)]\n",
    "print(\"Detected source columns:\", source_cols)\n",
    "\n",
    "def gather_sources(row) -> List[str]:\n",
    "    out = []\n",
    "    for c in source_cols:\n",
    "        v = row.get(c)\n",
    "        if pd.notna(v) and str(v).strip():\n",
    "            out.append(str(v))\n",
    "    return out\n",
    "\n",
    "df[\"sources_list\"] = df.apply(gather_sources, axis=1)\n",
    "df.head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ea8de5-dcc5-4c1b-a838-db55445805da",
   "metadata": {},
   "source": [
    "## 2.1) Test LLM call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f8f719d5-1d3c-4a66-b5cb-d44d4092f95e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"used_correct_sources\": true,\n",
      "  \"extracted_correct_insights\": true,\n",
      "  \"format_reasonable\": true,\n",
      "  \"answers_completely\": true,\n",
      "  \"needs_more_context\": false\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from openai import OpenAI\n",
    "client = OpenAI()  # assumes OPENAI_API_KEY is set\n",
    "\n",
    "row = df.iloc[0]\n",
    "question = row[\"question\"]\n",
    "ai_answer = row[\"conveyor_ai_answer\"]\n",
    "sources = [str(row[c]) for c in df.columns if c.startswith(\"source_\") and str(row[c]).strip() and not pd.isna(row[c])]\n",
    "\n",
    "payload = json.dumps({\"question\": question, \"ai_answer\": ai_answer, \"sources\": sources}, ensure_ascii=False)\n",
    "\n",
    "resp = client.chat.completions.create(\n",
    "    model=os.getenv(\"LLM_MODEL_NAME\",\"gpt-5-nano\"),\n",
    "    messages=[\n",
    "        {\"role\":\"system\",\"content\": SYSTEM_PROMPT},\n",
    "        {\"role\":\"user\",\"content\": payload},\n",
    "    ],\n",
    "    response_format={\"type\":\"json_object\"},\n",
    ")\n",
    "print(resp.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14aa8fe0",
   "metadata": {},
   "source": [
    "## 2.2) LLM feature extraction (synchronous, one call per row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14005ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "SYSTEM_PROMPT = \"\"\"You are an evaluation assistant for a RAG system.\n",
    "Given a question, an AI-generated answer, and a list of retrieved sources (strings),\n",
    "return **strict JSON** with ONLY these boolean keys:\n",
    "- used_correct_sources\n",
    "- extracted_correct_insights\n",
    "- format_reasonable\n",
    "- answers_completely\n",
    "- needs_more_context\n",
    "Interpret:\n",
    "- used_correct_sources: correct citations appear sufficient & relevant\n",
    "- extracted_correct_insights: answer's claims match the sources\n",
    "- format_reasonable: output format/structure fits the question\n",
    "- answers_completely: question is fully addressed\n",
    "- needs_more_context: more info would materially change/improve answer\n",
    "Return ONLY a JSON object with those five keys, boolean values.\"\"\"\n",
    "\n",
    "def build_user_payload(question: str, ai_answer: str, sources: List[str]) -> str:\n",
    "    return json.dumps({\"question\": question or \"\", \"ai_answer\": ai_answer or \"\", \"sources\": sources or []}, ensure_ascii=False)\n",
    "\n",
    "def validate_feature_json(txt: str) -> Dict[str, bool]:\n",
    "    try:\n",
    "        data = json.loads(txt)\n",
    "    except Exception:\n",
    "        return {}\n",
    "    keys = [\"used_correct_sources\",\"extracted_correct_insights\",\"format_reasonable\",\"answers_completely\",\"needs_more_context\"]\n",
    "    return {k: bool(data.get(k, False)) for k in keys}\n",
    "\n",
    "def llm_extract_row_sync(question: str, ai_answer: str, sources: List[str]) -> Dict[str, bool]:\n",
    "    if client is None:\n",
    "        # Fallback stub: simple heuristics so pipeline runs without API\n",
    "        complete = len(str(ai_answer).strip()) > 0\n",
    "        many_sources = len(sources or []) >= 2\n",
    "        return {\n",
    "            \"used_correct_sources\": many_sources,\n",
    "            \"extracted_correct_insights\": complete,\n",
    "            \"format_reasonable\": complete,\n",
    "            \"answers_completely\": complete,\n",
    "            \"needs_more_context\": not many_sources and not complete\n",
    "        }\n",
    "    try:\n",
    "        uprompt = build_user_payload(question, ai_answer, sources or [])\n",
    "        resp = client.chat.completions.create(\n",
    "            model=MODEL_NAME,\n",
    "            messages=[{\"role\":\"system\",\"content\": SYSTEM_PROMPT},\n",
    "                      {\"role\":\"user\",\"content\": uprompt}],\n",
    "            response_format={\"type\":\"json_object\"}\n",
    "        )\n",
    "        txt = resp.choices[0].message.content\n",
    "        return validate_feature_json(txt)\n",
    "    except Exception as e:\n",
    "        # Conservative fallback on errors\n",
    "        return {\"used_correct_sources\": False, \"extracted_correct_insights\": False, \"format_reasonable\": False, \"answers_completely\": False, \"needs_more_context\": True}\n",
    "\n",
    "def extract_features_sync(frame: pd.DataFrame, sleep_seconds: float = 0.0) -> pd.DataFrame:\n",
    "    records = frame.to_dict(orient=\"records\")\n",
    "    feats: List[Dict[str, bool]] = []\n",
    "    for i, r in enumerate(records, 1):\n",
    "        f = llm_extract_row_sync(r.get(QUESTION_COL, \"\"), r.get(AI_ANSWER_COL, \"\"), r.get(\"sources_list\", []))\n",
    "        feats.append(f)\n",
    "        if sleep_seconds > 0:\n",
    "            time.sleep(sleep_seconds)\n",
    "        if i % 25 == 0:\n",
    "            print(f\"Processed {i}/{len(records)} rows\")\n",
    "    feat_df = pd.DataFrame(feats)\n",
    "    return pd.concat([frame.reset_index(drop=True), feat_df.reset_index(drop=True)], axis=1)\n",
    "\n",
    "out_df = extract_features_sync(df, sleep_seconds=SLEEP_BETWEEN_SECONDS)\n",
    "out_df.head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370838e0",
   "metadata": {},
   "source": [
    "## 3) Labels: 1 if `conveyor_ai_grade == perfect`, else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca306cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def label_from_grade(s) -> int:\n",
    "    return 1 if str(s).strip().lower() == \"perfect\" else 0\n",
    "\n",
    "out_df[\"label\"] = out_df[GRADE_COL].apply(label_from_grade).astype(int)\n",
    "print(out_df[GRADE_COL].value_counts(dropna=False))\n",
    "print(\"Label counts:\", out_df[\"label\"].value_counts().to_dict())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f538265f",
   "metadata": {},
   "source": [
    "## 4) Train Logistic Regression on full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e67658",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "feature_cols = [\"used_correct_sources\",\"extracted_correct_insights\",\"format_reasonable\",\"answers_completely\",\"needs_more_context\"]\n",
    "X = out_df[feature_cols].astype(int).values\n",
    "y = out_df[\"label\"].values\n",
    "\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "clf.fit(X, y)\n",
    "\n",
    "out_df[\"pred_proba\"] = clf.predict_proba(X)[:,1]\n",
    "out_df[\"pred_label\"] = (out_df[\"pred_proba\"] >= 0.5).astype(int)\n",
    "\n",
    "print(\"Coefficients:\")\n",
    "for name, w in zip(feature_cols, clf.coef_[0]):\n",
    "    print(f\"{name:28s} {w:+.3f}\")\n",
    "print(\"Intercept:\", f\"{clf.intercept_[0]:+.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1778db",
   "metadata": {},
   "source": [
    "## 5) Confusion Matrix (threshold 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46291d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cm = confusion_matrix(out_df[\"label\"], out_df[\"pred_label\"])\n",
    "print(\"Confusion matrix:\\n\", cm)\n",
    "print(\"\\nClassification report:\\n\", classification_report(out_df[\"label\"], out_df[\"pred_label\"], digits=3))\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(cm, interpolation='nearest')\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "for (i, j), v in np.ndenumerate(cm):\n",
    "    plt.text(j, i, str(v), ha='center', va='center')\n",
    "plt.xticks([0,1],[0,1])\n",
    "plt.yticks([0,1],[0,1])\n",
    "plt.colorbar()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4081f8",
   "metadata": {},
   "source": [
    "## 6) Predicted probability vs original grade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d38e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cats = out_df[GRADE_COL].astype(str).fillna(\"NA\")\n",
    "unique_cats = sorted(cats.unique())\n",
    "groups = [out_df.loc[cats == c, \"pred_proba\"].values for c in unique_cats]\n",
    "\n",
    "plt.figure()\n",
    "plt.boxplot(groups, labels=unique_cats, showmeans=True)\n",
    "plt.title(\"Predicted p(accurate) by original grade\")\n",
    "plt.ylabel(\"Predicted probability\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
